# InfluxDB pipeline: same logic as ETL flow but source is InfluxDB instead of CSV.
# - Tick every 10s (tick_interval); each tick uses lookback 10s, stateful pod→chunk batching, merge by vincode, then send to aggregated compact topic.
# - Create topic with cleanup.policy=compact (see config/kafka_config). Key = vincode for compaction.
# Set url, token, org, bucket and pods to match your InfluxDB. Run: BENTO_CONFIG=./config/pipeline_influxdb.yaml go run .
input:
  influxdb:
    url: "http://localhost:8086"
    token: "${INFLUX_TOKEN}"
    org: "my-org"
    bucket: "telemetry"
    pods:
      - "pod-1"
      - "pod-2"
    chunk_duration: "2s"
    lookback: "10s"
    tick_interval: "10s"
    resource_map_path: "./config/resource_matrix.json"
    batch_size: 5000
    measurement: "telemetry"
    # optional: fault tolerance (retry InfluxDB query)
    # retry_max_attempts: 3
    # retry_initial_interval: "1s"
    # retry_max_interval: "30s"
    # optional: idempotency Option A — persist cycle end to avoid reprocessing on restart (e.g. 1 replica or shared volume)
    # checkpoint_path: "/data/influxdb_checkpoint"

pipeline:
  processors:
    - telemetry_aggregator:
        resource_matrix_path: "config/resource_matrix.json"
    - kafka_message_builder: {}

output:
  kafka_franz:
    seed_brokers:
      - localhost:19091
      - localhost:19092
      - localhost:19093
    topic: sensor-service.dispatch.telemetry-aggregated
    client_id: bento_influxdb
    key: ${! meta("vincode") }
